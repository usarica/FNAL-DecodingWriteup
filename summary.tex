\section{Summary and outlook}

Neural networks are promising error decoders for quantum circuits, providing the promise to learn complex qubit readout parameters and error patterns that are difficult to quantify experimentally, or algebraically through non-NN approaches.
In order to limit the duration of quantum error correction cycles to $\ll 1\mus$, these NNs need to run on hardware acceleration devices such as FPGAs, GPUs, and other specialized ASICs. The state-of-the-art FPGAs can typically accept only $\Order(100,000)$ parameters, and the development of specialized ASICs will likely face similar limits. For this reason, large NNs currently require a more involved room-temperature computational support system, which would increase the costs to make quantum computing systems commercially available.

The primary goal of this paper is to design a compact decoder NN architecture to maximize the number of NN parameters shared and keep the parameter count within the limits of a typical FPGA. The key novel feature of our architecture is the accounting of the spatial symmetries of the surface code in a recurrent structure, and using minimal additional features, we have achieved decoding performance that is competitive with the MWPM-based PyMatching~\cite{Higgott:2023} algorithm while only requiring $\leq72917$ NN parameters.

Since our primary focus has been to find a method to account for the rotational and translational symmetries, we leave a more thorough implementation of accounting for nonuniformities in hardware noise to a future study, providing only a simplistic option in our architecture, as mentioned in Section~\ref{sec:kerncomb}. Some of the mathematical formulations explored in this architecture may also need to be modified in order to improve decoder performance over larger $d$ or $r$ values.

An anticipated area of investigation for the further development of the presented architecture is the inclusion of additional error-sensitive data from qubit readout, \eg, I-Q information as exemplified in Ref.~\cite{Bausch:2023jgi}. In this context, one could also consider in the farther future joint error correction and mitigation schemes, \eg, as conceptually explored in Ref.~\cite{Sarica:2023}.

Since our NN architecture is currently based on the TensorFlow Python API, which is a general NN framework not adequately optimized for the fast execution times required in QEC cycles, another future development stage would be to re-code the architecture using basic C/C++ for optimized implementation in FPGAs and other room-temperature hardware acceleration devices, through \textsc{hls4ml}~\cite{hls4ml} or other language synthesis frameworks.

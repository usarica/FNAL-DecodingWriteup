\section{Introduction}

Current approaches to quantum error correction (QEC) involve the design of dedicated neural network (NN) architectures to decode errors. These architectures provide potential capability to learn complex qubit readout parameters and error patterns that are difficult to quantify experimentally, and there is evolving expertise to run these NNs on hardware acceleration devices such as FPGAs, GPUs, and other specialized ASICs, which is needed to parallelize the computations and limit the duration of quantum error correction cycles to $\ll 1\mus$. It is estimated that FPGA implementations can potentially reduce delays from QEC cycles below $90\ns$ while dedicated ASICs can further reduce the time to $<30\ns$~\cite{Overwater:2022qwb}.

In another recent study~\cite{Bausch:2023jgi}, NNs running on the Google Sycamore chip have been shown to outperform other techniques based on minimum-weight perfect-matching (MWPM), \eg, PyMatching~\cite{Higgott:2023}, and tensor networks. This study presents a transformer-based recurrent, convolutional neural network architecture that requires slightly over five million trainable NN parameters, and the architecture implements the capability to process flattened qubit I-Q readout inputs to further enhance error detection accuracy.

Running error decoding NNs on FPGAs or dedicated ASICs requires the mathematical operations and the number of parameters used in the NN to be as compact as possible, so the NN architecture needs to be designed carefully. The state-of-the-art FPGAs can typically accept only $\Order(100,000)$ parameters, so NNs with over a million parameters would need a more involved room-temperature computational support system to decode errors in real time, thereby increasing the costs for a commercial quantum computing system.

For this reason, our primary goal in this paper is to outline an alternative approach to design the decoder NN architecture by maximizing the number of NN parameters shared. While our investigation does not yet take additional input from hardware readout into account, we would naively expect only a modest increase in the number of parameters to account for such additional input, likely with a growth factor quadratic in terms of the distance parameter of the QEC code. Since our primary focus has been to find a method to account for the rotational and translational symmetries, we leave a more thorough implementation of accounting for nonuniformities in hardware noise to a future study, providing only a simplistic option in our architecture as an example avenue to explore.

Within these limitations, minimal settings in our NN architecture can reduce the number of parameters to $\leq 72917$ for all surface codes, which is close to a two orders of magnitude improvement with respect to Ref.~\ref{Bausch:2023jgi}, while maintaining decoder performance reaching the levels of PyMatching~\cite{Higgott:2023}. For distance-5 surface codes, our NN decoder is on par with this algorithm in a subset of its operational modes. The code for our compact recursive, convolutional NN (RCNN) architecture can be found in this Github repository~\cite{ourcode} and is based on the TensorFlow Python API~\cite{tensorflow,tensorflow2}.

Throughout the rest of the paper, we will use the following glossary when describing the components of the neural and relating them to a QEC workflow:
\begin{itemize}
\item Round: A single, complete set of stabilizer measurements over the circuit that only leaves logical observables as additional degrees of freedom.
\item Cycle: A set of stabilizer measurement rounds upon which either error correction is applied (or errors are tracked) or the data qubits are measured. We will reserve the symbol $r$ to refer to the total number of rounds in a cycle.
\item Kernel distance, $k$: The distance of the underlying surface code that is used in the convolution kernels of the NN architecture. The kernel construct is described in Section~\ref{sec:kernels}, and we will use the value $k=3$ for most of the discussion in this paper.
\item Surface code with distance $d$: When referring to surface codes, we will explicitly refer to rotated surface codes~\cite{Bombin:2007}. While the implementation of our NN architecture assumes this configuration, future investigations could easily account for unrotated surface codes or other structured codes, \eg, the more general class of low-density parity-check codes, using similar concepts.
\item $z$-like variable: A real-valued NN parameter or derived variable that can take values within $(-\infty, \infty)$. For most of the $x$-like, or $p$- and $f$-like variables, described below, that are obtained from an underlying $z$-like value, this $z$-like value is clipped between $\pm12$ in order to avoid singularities in derivatives, which arise fundamentally from the floating-point precision issue $e^z \to 0$ (1) for very small (large) signed values of $z$.
\item $x$-like variable: A real-valued transformed NN variable that can take values within $[0, \infty)$. As done in this paper, one can obtain such values using the transformation $x(z)=e^z$, with the useful property that $x(-z)=1/x(z)$.
\item Sigmoid activation/transformation: There are many options in the literature to transform a $z$-like variable into values bounded asymptotically as $z \to \pm \infty$. When we use this term in this paper, we will explicitly refer to the transformation $\sigma(z)=\left(1+e^{-z}\right)^{-1}$, with the useful property that $\sigma(-z)=1-\sigma(z)$.
\item $p$- (probability) or $f$- (fraction) like variable: A real-valued NN variable that can take values within $(0, 1)$, or $[0,1]$ if the bounds are included explicitly in an embedding parametrization. In the case of exclusive bounds, these variables are obtained using a sigmoid transformation.
\item $c_\varphi$- or $\alpha$-like parameters: A real-valued NN variable that can take values within $(-1, 1)$. These variables can be obtained via a hyperbolic tangent transformation over $z$-like trainable variables. While the way to obtain these parameters are the same, we will distinguish their meaning as the cosine of a phase difference for $c_\varphi$, and a $z$-like variable sign inversion control parameter for $\alpha$.
\item Detectors/detector events: A set of measurements that can be used to flag potential errors. The formalism to define detector events is explained in Refs.~\cite{Gidney:2021,Higgott:2023,Bausch:2023jgi}. One example for detectors would be an XOR of the readouts of the same measure qubits over two consecutive rounds. While stabilizer measurement values are denoted with the reserved letter $m$, detector events are denoted with the reserved letter $e$ throughout this paper, unless it is clear from the context that we are referring to exponentiation using Euler's number.
\end{itemize}

The logical flow of the paper is as follows: In Section~\ref{sec:kernelconstruct}, we describe the kernel concept that maximizes the sharing of NN weights and respects the approximate (exact if hardware noise were uniform) rotational and translational symmetries of the circuit. Section~\ref{sec:kerncomb} is dedicated to the discussion of how the output of kernels is combined, and Section~\ref{sec:rcnn} describes how our kernel concept can be utilized for a recurrent NN architecture. We provide a final summary of our findings and outlook for future investigations in Section~\ref{sec:summary}.
